{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Deploy to an batch endpoint\n",
        "\n",
        "Imagine a health clinic takes patient measurements all day, saving the details for each patient in a separate file. Then overnight, the diabetes prediction model can be used to process all of the day's patient data as a batch, generating predictions that will be waiting the following morning so that the clinic can follow up with patients who are predicted to be at risk of diabetes. With Azure Machine Learning, you can accomplish this by creating a batch endpoint; and that's what you'll implement in this exercise.\n",
        "\n",
        "## Before you start\n",
        "\n",
        "You'll need the latest version of the  **azure-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
        "\n",
        "> **Note**:\n",
        "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip show azure-ai-ml"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: azure-ai-ml\r\nVersion: 1.25.0\r\nSummary: Microsoft Azure Machine Learning Client Library for Python\r\nHome-page: https://github.com/Azure/azure-sdk-for-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: azuresdkengsysadmins@microsoft.com\r\nLicense: MIT License\r\nLocation: /anaconda/envs/azureml_py38/lib/python3.10/site-packages\r\nRequires: azure-common, azure-core, azure-mgmt-core, azure-monitor-opentelemetry, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, msrest, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\r\nRequired-by: \r\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1740323011765
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1740323055150
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1740323057656
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## create the model\n",
        "\n",
        "- Customize the model with a defined signature\n",
        "\n",
        "You can manually log the model using `mlflow.sklearn.log_model`. You'll also create a signature manually. And finally, you'll log the scikit-learn model.\n",
        "\n",
        "Run the following cell to create the **train-model-signature.py** script in the **src** folder. The script trains a classification model by using the **accident.csv** file in the same folder, which is passed as an argument. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "src folder created\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1740323059890
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import mlflow"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1740323115713
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stores = ml_client.datastores.list()\n",
        "for ds_name in stores:\n",
        "    print(ds_name.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "workspacefilestore\nworkspaceworkingdirectory\nworkspaceblobstore\nworkspaceartifactstore\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1740317896576
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the dataset (Ensure the file exists in the directory)\n",
        "try:\n",
        "    df = pd.read_csv('accident.csv')\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"accident.csv not found in the current directory\")\n",
        "\n",
        "df['Age'] = df['Age'].astype(float)\n",
        "df['Speed_of_Impact'] = df['Speed_of_Impact'].astype(float)\n",
        "df = df.dropna().copy()\n",
        "df.head(5)\n",
        "\n",
        "# Define categorical and numeric columns\n",
        "cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "num_columns = ['Age', 'Speed_of_Impact']\n",
        "\n",
        "# Define feature transformations\n",
        "numeric_transformer = make_pipeline(\n",
        "    SimpleImputer(strategy=\"mean\"),\n",
        "    StandardScaler()\n",
        ")\n",
        "\n",
        "categorical_transformer = make_pipeline(\n",
        "    SimpleImputer(strategy=\"most_frequent\"),\n",
        "    OneHotEncoder(drop='first')\n",
        ")\n",
        "\n",
        "# Combine transformations\n",
        "features_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numeric\", numeric_transformer, num_columns),\n",
        "        (\"categorical\", categorical_transformer, cat_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Extract features and labels\n",
        "X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "y = df['Survived'].values\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "# Apply transformations\n",
        "X_train_transformed = features_transformer.fit_transform(X_train)\n",
        "X_test_transformed = features_transformer.transform(X_test)\n",
        "\n",
        "# Define path for saving transformer\n",
        "transformer_path = os.path.join('./src', \"features_transformer.joblib\")\n",
        "\n",
        "# Save the feature transformer for future use\n",
        "joblib.dump(features_transformer, transformer_path)\n",
        "print(f\"Feature transformer saved at: {transformer_path}\")\n",
        "\n",
        "print(\"Training model...\")\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(C=1/0.01, solver=\"liblinear\")\n",
        "model.fit(X_train_transformed, y_train)  # Use Transformed Data\n",
        "\n",
        "# Make predictions\n",
        "y_hat = model.predict(X_test_transformed)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "\n",
        "# Calculate AUC Score\n",
        "y_scores = model.predict_proba(X_test_transformed)  # Compute Probabilities\n",
        "auc = roc_auc_score(y_test, y_scores[:, 1])\n",
        "print(\"AUC Score:\", auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:, 1])\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line\n",
        "plt.plot(fpr, tpr)  # ROC curve\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.close()\n",
        "\n",
        "# Define path for saving model\n",
        "# Log the model and feature transformer in MLflow\n",
        "#mlflow.sklearn.log_model(\n",
        "    #sk_model=model,\n",
        "    #artifact_path=\"model\"  # This is the relative path within MLflow's artifact storage\n",
        "#)\n",
        "\n",
        "# Optional: Save as MLflow model format locally\n",
        "mlflow.sklearn.save_model(\n",
        "    sk_model=model,\n",
        "    path=\"model\"\n",
        ")\n",
        "\n",
        "print(\"Model saved locally \")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Feature transformer saved at: ./src/features_transformer.joblib\nTraining model...\nAccuracy: 0.5254237288135594\nAUC Score: 0.5816091954022988\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/_distutils_hack/__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n  warnings.warn(\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1740309375949
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the model\n",
        "\n",
        "Batch deployments can only deploy models registered in the workspace. You'll register an MLflow model, which is stored in the local `model` folder. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "model_name = \"accident-survival-model\"\n",
        "model = ml_client.models.create_or_update(\n",
        "    Model(name=model_name, path='./model', type=AssetTypes.MLFLOW_MODEL)\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading model (0.0 MBs): 100%|██████████| 1929/1929 [00:00<00:00, 32894.96it/s]\n\u001b[39m\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1740309567947
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "my_path = './src/features_transformer.joblib'\n",
        "\n",
        "transformer = Data(\n",
        "    path=my_path,\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    description=\"Data asset pointing to a pkl file for transforming, automatically uploaded to the default datastore\",\n",
        "    name=\"accident-transformer\"\n",
        ")\n",
        "\n",
        "ml_client.data.create_or_update(transformer)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading features_transformer.joblib\u001b[32m (< 1 MB): 0.00B [00:00, ?B/s]\r\u001b[32mUploading features_transformer.joblib\u001b[32m (< 1 MB): 100%|██████████| 4.85k/4.85k [00:00<00:00, 308kB/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "Data({'path': 'azureml://subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/gabby102/workspaces/health-update/datastores/workspaceblobstore/paths/LocalUpload/6812fe07f6f59ac0e574121d1e03e0d9/features_transformer.joblib', 'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'accident-transformer', 'description': 'Data asset pointing to a pkl file for transforming, automatically uploaded to the default datastore', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/gabby102/providers/Microsoft.MachineLearningServices/workspaces/health-update/data/accident-transformer/versions/1', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f375f1880d0>, 'serialize': <msrest.serialization.Serializer object at 0x7f375f48cca0>, 'version': '1', 'latest_version': None, 'datastore': None})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1740309591221
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('accident.csv')\n",
        "experiment = df.head().drop('Survived', axis = 1)\n",
        "experiment"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "   Age  Gender  Speed_of_Impact Helmet_Used Seatbelt_Used\n0   56  Female             27.0          No            No\n1   69  Female             46.0          No           Yes\n2   46    Male             46.0         Yes           Yes\n3   32    Male            117.0          No           Yes\n4   60  Female             40.0         Yes           Yes",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Speed_of_Impact</th>\n      <th>Helmet_Used</th>\n      <th>Seatbelt_Used</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>56</td>\n      <td>Female</td>\n      <td>27.0</td>\n      <td>No</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>69</td>\n      <td>Female</td>\n      <td>46.0</td>\n      <td>No</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>46</td>\n      <td>Male</td>\n      <td>46.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>32</td>\n      <td>Male</td>\n      <td>117.0</td>\n      <td>No</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>Female</td>\n      <td>40.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1740317847968
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get the registered data asset\n",
        "asset = ml_client.data.get(name=\"accident-transformer\", version=\"1\")\n",
        "asset.path"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "'azureml://subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/gabby102/workspaces/health-update/datastores/workspaceblobstore/paths/LocalUpload/6812fe07f6f59ac0e574121d1e03e0d9/features_transformer.joblib'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1740317922864
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import joblib\n",
        "import fsspec\n",
        "\n",
        "# 1️ Disable OpenTelemetry before anything else\n",
        "os.environ[\"OTEL_PYTHON_DISABLED\"] = \"true\"\n",
        "os.environ[\"OTEL_TRACES_EXPORTER\"] = \"none\"\n",
        "os.environ[\"OTEL_METRICS_EXPORTER\"] = \"none\"\n",
        "os.environ[\"OTEL_LOGS_EXPORTER\"] = \"none\"\n",
        "os.environ[\"OTEL_PROPAGATORS\"] = \"none\"\n",
        "os.environ[\"OTEL_PYTHON_LOG_LEVEL\"] = \"ERROR\"\n",
        "\n",
        "# 2 Suppress warnings from OpenTelemetry and others\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"Attempting to instrument while already instrumented\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Overriding of current TracerProvider is not allowed\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Overriding of current LoggerProvider is not allowed\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Overriding of current MeterProvider is not allowed\")\n",
        "\n",
        "#  Unload OpenTelemetry modules if they are loaded\n",
        "for module in list(sys.modules.keys()):\n",
        "    if \"opentelemetry\" in module:\n",
        "        del sys.modules[module]\n",
        "\n",
        "#  Load the `.pkl` transformer file safely\n",
        "uri = f\"azureml://subscriptions/{ml_client.subscription_id}/resourceGroups/{ml_client.resource_group_name}/workspaces/{ml_client.workspace_name}/data/{asset.name}/versions/{asset.version}\"\n",
        "uri\n",
        "\n",
        "try:\n",
        "    with fsspec.open(uri, \"rb\") as f:\n",
        "        features_transformer = joblib.load(f)\n",
        "    print(\"Feature transformer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading transformer: {e}\")\n",
        "\n",
        "#  Apply the transformation to the new data\n",
        "try:\n",
        "    transformed_data = features_transformer.transform(experiment)\n",
        "    print(\"Transformation applied successfully!\")\n",
        "    print(transformed_data)\n",
        "except Exception as e:\n",
        "    print(f\"Error during transformation: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Feature transformer loaded successfully!\nTransformation applied successfully!\n[[ 0.83358372 -1.37479361  0.          0.          0.        ]\n [ 1.68976168 -0.75843333  0.          0.          1.        ]\n [ 0.17498528 -0.75843333  1.          1.          1.        ]\n [-0.74705253  1.5448077   1.          0.          1.        ]\n [ 1.09702309 -0.95307342  0.          1.          1.        ]]\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1740309674271
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "asset = ml_client.data.get(name=\"accident-transformer\", version=\"1\")\n",
        "asset\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "Data({'path': 'azureml://subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/gabby102/workspaces/health-update/datastores/workspaceblobstore/paths/LocalUpload/6812fe07f6f59ac0e574121d1e03e0d9/features_transformer.joblib', 'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'accident-transformer', 'description': 'Data asset pointing to a pkl file for transforming, automatically uploaded to the default datastore', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/gabby102/providers/Microsoft.MachineLearningServices/workspaces/health-update/data/accident-transformer/versions/1', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fc97435db70>, 'serialize': <msrest.serialization.Serializer object at 0x7fc9a055e320>, 'version': '1', 'latest_version': None, 'datastore': None})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1740321639418
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azureml.core import Workspace\n",
        "import azure.ai.ml._artifacts._artifact_utilities as artifact_utils\n",
        "from azure.identity import DefaultAzureCredential,InteractiveBrowserCredential\n",
        "\n",
        "# Create a local directory to download the asset\n",
        "download_dir = \"./temp_transformer\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "asset = ml_client.data.get(name=\"accident-transformer\", version=\"1\")\n",
        "artifact_utils.download_artifact_from_aml_uri(uri = asset.path, destination = download_dir, datastore_operation=ml_client.datastores)\n",
        "# Determine the local file name by taking the basename of the asset's path.\n",
        "local_file = os.path.join(download_dir, os.path.basename(asset.path))\n",
        "print(\"Local file path:\", local_file)\n",
        "\n",
        "# Load the joblib transformer from the downloaded file.\n",
        "with open(local_file, \"rb\") as f:\n",
        "    features_transformer = joblib.load(f)\n",
        "\n",
        "print(\"Feature transformer loaded successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Local file path: ./temp_transformer/features_transformer.joblib\nFeature transformer loaded successfully!\n"
        }
      ],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1740322230031
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare the data\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('accident.csv').dropna()\n",
        "df['Age'] = df['Age'].astype(float)\n",
        "df['Speed_of_Impact'] = df['Speed_of_Impact'].astype(float)\n",
        "\n",
        "# Drop the target column 'Survived'\n",
        "df = df.drop(columns=['Survived'])\n",
        "\n",
        "# Split the dataframe into 5 equal parts\n",
        "split_dfs = np.array_split(df, 5)\n",
        "\n",
        "# Create the \"data\" folder if it doesn't exist\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Save each split dataset into the \"data\" folder\n",
        "for i, split_df in enumerate(split_dfs):\n",
        "    file_path = f\"data/accident_part_{i+1}.csv\"\n",
        "    split_df.to_csv(file_path, index=False)\n",
        "    print(f\" Saved: {file_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " Saved: data/accident_part_1.csv\n Saved: data/accident_part_2.csv\n Saved: data/accident_part_3.csv\n Saved: data/accident_part_4.csv\n Saved: data/accident_part_5.csv\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1740309701377
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_data = pd.read_csv('data/accident_part_1.csv')\n",
        "first_data.columns\n",
        "# Define required columns\n",
        "required_columns = ['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']\n",
        "\n",
        "# Check for missing columns\n",
        "for col in required_columns:\n",
        "    if col not in first_data.columns:\n",
        "        print(f\"Column '{col}' is not present in the dataset.\")"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1740309743180
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('./data')\n",
        "f = os.listdir('./data')\n",
        "f"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "['accident_part_1.csv',\n 'accident_part_2.csv',\n 'accident_part_3.csv',\n 'accident_part_4.csv',\n 'accident_part_5.csv']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1740310697417
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the directory\n",
        "f = os.listdir('./data')\n",
        "\n",
        "# Iterate through the files and filter only CSV files\n",
        "for i in f:\n",
        "    if i.endswith('.csv'):\n",
        "        print(i)  # Prints only CSV files\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "accident_part_1.csv\naccident_part_2.csv\naccident_part_3.csv\naccident_part_4.csv\naccident_part_5.csv\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1740309776500
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/transform_data.py\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import argparse\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "import azure.ai.ml._artifacts._artifact_utilities as artifact_utils\n",
        "\n",
        "def get_data(ml_client, input_dir, feature_transformer_name):\n",
        "    \"\"\"\n",
        "    Reads CSV files from input_dir, applies the feature transformer asset,\n",
        "    and returns a list of transformed DataFrames.\n",
        "    \"\"\"\n",
        "    required_columns = ['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    all_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
        "    missing_columns = []\n",
        "    dataframes = []\n",
        "    \n",
        "    # Load the joblib transformer asset\n",
        "    try:\n",
        "        asset = ml_client.data.get(name=feature_transformer_name, version=\"1\")\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            # Download the artifact into the temporary directory.\n",
        "            # Note: We use temp_dir as the destination directory.\n",
        "            artifact_utils.download_artifact_from_aml_uri(\n",
        "                uri=asset.path,\n",
        "                destination=temp_dir,\n",
        "                datastore_operation=ml_client.datastores\n",
        "            )\n",
        "            # Construct a candidate local path by joining the temp_dir and basename of the asset's path.\n",
        "            candidate_path = os.path.join(temp_dir, os.path.basename(asset.path))\n",
        "            print(\"Downloaded candidate path:\", candidate_path)\n",
        "            \n",
        "            # Check if the candidate_path is a directory.\n",
        "            if os.path.isdir(candidate_path):\n",
        "                files_in_dir = os.listdir(candidate_path)\n",
        "                if not files_in_dir:\n",
        "                    raise Exception(\"Downloaded directory is empty.\")\n",
        "                # If only one file exists, use that; otherwise, search for a .joblib file.\n",
        "                if len(files_in_dir) == 1:\n",
        "                    local_file = os.path.join(candidate_path, files_in_dir[0])\n",
        "                else:\n",
        "                    joblib_files = [f for f in files_in_dir if f.endswith('.joblib')]\n",
        "                    if not joblib_files:\n",
        "                        raise Exception(\"No joblib file found in the downloaded directory.\")\n",
        "                    local_file = os.path.join(candidate_path, joblib_files[0])\n",
        "            else:\n",
        "                local_file = candidate_path\n",
        "            \n",
        "            print(\"Using local file:\", local_file)\n",
        "            with open(local_file, \"rb\") as f:\n",
        "                features_transformer = joblib.load(f)\n",
        "            print(\"Feature transformer loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading transformer: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Process each CSV file\n",
        "    for file in all_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Check if all required columns are present\n",
        "        for col in required_columns:\n",
        "            if col not in df.columns and col not in missing_columns:\n",
        "                missing_columns.append(col)\n",
        "\n",
        "        # Apply the transformation to the data\n",
        "        try:\n",
        "            transformed_data = features_transformer.transform(df)\n",
        "            transformed_df = pd.DataFrame(transformed_data, columns=required_columns)\n",
        "            print(f\"Transformation applied successfully on {file}!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during transformation for {file}: {e}\")\n",
        "            continue\n",
        "\n",
        "        dataframes.append(transformed_df)\n",
        "    \n",
        "    if missing_columns:\n",
        "        print(f\"Missing columns: {missing_columns}\")\n",
        "        return None\n",
        "    return dataframes\n",
        "\n",
        "def main(args):\n",
        "    try:\n",
        "        credential = DefaultAzureCredential()\n",
        "        ml_client = MLClient(\n",
        "            credential=credential,\n",
        "            subscription_id=\"cda9116f-5326-4a9b-9407-bc3a4391c27c\",\n",
        "            resource_group_name=\"gabby102\",\n",
        "            workspace_name=\"health-update\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Default credential failed: {e}\")\n",
        "        print(\"Falling back to interactive browser login...\")\n",
        "        try:\n",
        "            credential = InteractiveBrowserCredential()\n",
        "            ml_client = MLClient(\n",
        "                credential=credential,\n",
        "                subscription_id=\"cda9116f-5326-4a9b-9407-bc3a4391c27c\",\n",
        "                resource_group_name=\"gabby102\",\n",
        "                workspace_name=\"health-update\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Interactive login failed: {e}\")\n",
        "            sys.exit(1)\n",
        "    \n",
        "    datas = get_data(ml_client, args.input_data, args.feature_transformer_name)\n",
        "    if datas is None or len(datas) == 0:\n",
        "        print(\"Error: No data was transformed successfully\")\n",
        "        sys.exit(1)\n",
        "    \n",
        "    os.makedirs(args.output_data, exist_ok=True)\n",
        "    try:\n",
        "        for i, split_df in enumerate(datas):\n",
        "            output_path = Path(args.output_data) / f\"accident_part_{i+1}.csv\"\n",
        "            split_df.to_csv(output_path, index=False)\n",
        "            print(f\"Saved: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving transformed data: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Transform accident data in Azure ML\")\n",
        "    parser.add_argument(\n",
        "        \"--input_data\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        default=\"./data\",\n",
        "        help=\"Path to the input data directory containing CSV files\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_data\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path where the transformed data will be written\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--feature_transformer_name\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        default=\"accident-transformer\",\n",
        "        help=\"Name of the feature transformer asset\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"*\" * 60)\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n",
        "    print(\"*\" * 60 + \"\\n\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/transform_data.py\n"
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/batch_scoring.py\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import mlflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "import glob\n",
        "\n",
        "def init():\n",
        "    \"\"\"Initialize the scoring environment by loading the model.\"\"\"\n",
        "    global model\n",
        "    try:\n",
        "        model_path = os.path.join(os.environ[\"AZUREML_MODEL_DIR\"], \"model\")\n",
        "        model = mlflow.pyfunc.load_model(model_path)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "def run(training_data_path: str) -> List[str]:\n",
        "    try:\n",
        "        results = []\n",
        "        # List all CSV files in the input folder\n",
        "        csv_files = glob.glob(os.path.join(training_data_path, \"*.csv\"))\n",
        "        \n",
        "        if not csv_files:\n",
        "            logging.error(f\"No CSV files found in the folder: {training_data_path}\")\n",
        "            return results\n",
        "\n",
        "        for file_path in csv_files:\n",
        "            try:\n",
        "                # Read the CSV file\n",
        "                data = pd.read_csv(file_path)\n",
        "                \n",
        "                # Validate input data\n",
        "                expected_columns = ['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']\n",
        "                if not all(col in data.columns for col in expected_columns):\n",
        "                    raise ValueError(f\"Missing required columns in {file_path}\")\n",
        "                \n",
        "                input_data = data[expected_columns]\n",
        "                \n",
        "                # Make predictions\n",
        "                predictions = model.predict(input_data)\n",
        "                probabilities = model.predict_proba(input_data)\n",
        "                \n",
        "                # Validate predictions\n",
        "                if len(predictions) != len(data):\n",
        "                    raise ValueError(\"Prediction length mismatch\")\n",
        "                \n",
        "                # Create output dataframe\n",
        "                output_df = data.copy()\n",
        "                output_df['prediction'] = predictions\n",
        "                output_df['survival_probability'] = probabilities[:, 1]\n",
        "                \n",
        "                results.append(output_df.to_json(orient='records'))\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing file {file_path}: {str(e)}\")\n",
        "                continue\n",
        "                \n",
        "        return results\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in run: {str(e)}\")\n",
        "        raise\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/batch_scoring.py\n"
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/batch_endpoint.py\n",
        "\n",
        "import time\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml import MLClient,Input\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.entities import BatchEndpoint, BatchDeployment, Model, Environment, CodeConfiguration\n",
        "\n",
        "# Function to create batch endpoint if it doesn't exist\n",
        "def create_batch_endpoint(ml_client,endpoint_name):\n",
        "    try:\n",
        "        # Try to get existing endpoint\n",
        "        endpoint = ml_client.batch_endpoints.get(endpoint_name)\n",
        "        print(f\"Batch endpoint {endpoint_name} already exists.\")\n",
        "    except Exception:\n",
        "        # Create new endpoint\n",
        "        endpoint = BatchEndpoint(\n",
        "            name=endpoint_name,\n",
        "            description=\"Batch endpoint for accident prediction\",\n",
        "        )\n",
        "        ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "        print(f\"Created new batch endpoint: {endpoint_name}\")\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "# Function to create and deploy the batch deployment\n",
        "def create_batch_deployment(ml_client, endpoint_name,env_name, model_name, deployment_name,code_path,scoring_path,compute):\n",
        "    # Get the latest version of the model\n",
        "    model = ml_client.models.get(name=model_name, label=\"latest\")\n",
        "    \n",
        "    # Code configuration\n",
        "    code_configuration = CodeConfiguration(\n",
        "        code=code_path,  # Local path to the code directory\n",
        "        scoring_script=scoring_path  # The script that contains the init() and run() functions\n",
        "    )\n",
        "    # Create the deployment\n",
        "    deployment = BatchDeployment(\n",
        "        name=deployment_name,\n",
        "        description=\"Deployment for accident prediction model\",\n",
        "        endpoint_name=endpoint_name,\n",
        "        model=model,\n",
        "        environment=env_name , # Use the registered environment ID\n",
        "        code_configuration=code_configuration,\n",
        "        compute=compute,  # Replace with your compute target name\n",
        "        instance_count=1,\n",
        "        max_concurrency_per_instance=1,\n",
        "        mini_batch_size=10,\n",
        "        output_action=AssetTypes.MLFLOW_MODEL,\n",
        "        output_file_name=\"predictions.csv\",\n",
        "        retry_settings={\"max_retries\": 3, \"timeout\": 300},\n",
        "        logging_level=\"info\",\n",
        "    )\n",
        "\n",
        "    # Create or update the deployment\n",
        "    ml_client.batch_deployments.begin_create_or_update(deployment).result()\n",
        "    print(\"Deployment created successfully!\")\n",
        "\n",
        "    # Set the deployment as the default for the endpoint\n",
        "    endpoint = ml_client.batch_endpoints.get(endpoint_name)\n",
        "    endpoint.defaults.deployment_name = deployment_name\n",
        "    ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "    return deployment\n",
        "\n",
        "    # Function to create and submit a batch job\n",
        "def submit_batch_job(ml_client, endpoint_name,training_data_path,name_of_data):\n",
        "\n",
        "    # Register the data in Azure ML\n",
        "    data_path = training_data_path\n",
        "    dataset_name = name_of_data\n",
        "    dataset_in_use = Data(\n",
        "        path=data_path,\n",
        "        type=AssetTypes.URI_FOLDER,\n",
        "        description=\"An unlabeled dataset for survivor classification\",\n",
        "        name=dataset_name,\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        registered_data = ml_client.data.create_or_update(dataset_in_use)\n",
        "        print(f\"Dataset registered with name: {registered_data.name}\")\n",
        "        print(f\"Dataset version: {registered_data.version}\")\n",
        "        print(f\"Dataset registration path: {registered_data.path}\")\n",
        "        return registered_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error registering dataset: {e}\")\n",
        "        return None\n",
        "    else:\n",
        "        print(\"Could not proceed due to missing columns or transformation errors.\")\n",
        "        return None\n",
        "    \n",
        "    #get data \n",
        "    data_to_be_processed = ml_client.data.get(name=name_of_data, label=\"latest\")\n",
        "\n",
        "    # Create job input\n",
        "    job_input = Input(path=AssetTypes.URI_FOLDER, type=data_to_be_processed.id)\n",
        "    \n",
        "    # Submit the job\n",
        "    job = ml_client.batch_endpoints.invoke(\n",
        "        endpoint_name=endpoint_name,\n",
        "        input=job_input\n",
        "    )\n",
        "    # Monitor job progress\n",
        "    while True:\n",
        "        job_status = ml_client.jobs.get(job.name)\n",
        "        print(f\"Job status: {job_status.status}\")\n",
        "        \n",
        "        if job_status.status in ['Completed', 'Failed', 'Canceled']:\n",
        "            print(f\"Job finished with status: {job_status.status}\")\n",
        "            if job_status.status == 'Completed':\n",
        "                print(f\"Output available at: {job_status.outputs}\")\n",
        "            break\n",
        "            \n",
        "        time.sleep(30)  # Wait 30 seconds before checking again\n",
        "            \n",
        "        return job\n",
        "    print(f\"Submitted batch job with ID: {job.name}\")\n",
        "    return job\n",
        "\n",
        "# Main function\n",
        "def main(args):\n",
        "    # Get Azure ML client\n",
        "    # Get Azure ML client\n",
        "    try:\n",
        "        # Try DefaultAzureCredential first\n",
        "        credential = DefaultAzureCredential()\n",
        "        ml_client = MLClient(\n",
        "            credential=credential,\n",
        "            subscription_id=\"cda9116f-5326-4a9b-9407-bc3a4391c27c\",\n",
        "            resource_group_name=\"gabby102\",\n",
        "            workspace_name=\"health-update\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Default credential failed: {e}\")\n",
        "        print(\"Falling back to interactive browser login...\")\n",
        "        try:\n",
        "            # Fall back to interactive login\n",
        "            credential = InteractiveBrowserCredential()\n",
        "            ml_client = MLClient(\n",
        "                credential=credential,\n",
        "                subscription_id=\"cda9116f-5326-4a9b-9407-bc3a4391c27c\",\n",
        "                resource_group_name=\"gabby102\",\n",
        "                workspace_name=\"health-update\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Interactive login failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Create or get batch endpoint\n",
        "    endpoint = create_batch_endpoint(ml_client, args.endpoint_name)\n",
        "    # Create batch deployment if requested\n",
        "    deployment = create_batch_deployment(\n",
        "            ml_client, \n",
        "            args.endpoint_name, \n",
        "            args.env_name,\n",
        "            args.model_name, \n",
        "            args.deployment_name,\n",
        "            args.code_path,\n",
        "            args.scoring_path,\n",
        "            args.compute\n",
        "        )\n",
        "    # Submit batch job if requested\n",
        "    job = submit_batch_job(\n",
        "            ml_client, \n",
        "            args.endpoint_name,\n",
        "            args.training_data_path,\n",
        "            args.name_of_data)\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "# Run script\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up argument parser\n",
        "    parser = argparse.ArgumentParser(description='deploy batchpipeline in Azure ML')\n",
        "    \n",
        "    \n",
        "    parser.add_argument('--endpoint_name', type=str, default=\"accident-prediction-endpoint\",\n",
        "                       help='Name of the batch endpoint')\n",
        "\n",
        "    parser.add_argument('--model_name', type=str, required = True,\n",
        "                       help='Name of the registered model to use for batch deployment')\n",
        "    \n",
        "    parser.add_argument('--deployment_name', type=str, default=\"accident-prediction-deployment\",\n",
        "                       help='Name for the batch deployment')\n",
        "    \n",
        "    parser.add_argument('--env_name', type=str, default=\"accident-prediction-environment\",\n",
        "                       help='name of already created environment')\n",
        "    \n",
        "    parser.add_argument('--code_path', type=str, default=\"./src\",\n",
        "                       help='Path to the code directory')\n",
        "    parser.add_argument('--scoring_path', type=str, default=\"batch_scoring.py\",\n",
        "                       help='Name of the scoring script')\n",
        "    parser.add_argument('--compute', type=str, default=\"captgt0071\",\n",
        "                       help='Name of the compute target')\n",
        "    parser.add_argument('--training_data_path', type=str,\n",
        "                       help='the link from the output of  transforming data ')\n",
        "\n",
        "    \n",
        "    parser.add_argument('--name_of_data', type=str,default = 'accident_survival_data',\n",
        "                       help='Name of the registered dataset to use for the job')\n",
        "    \n",
        "    \n",
        "    print(\"\\n\" + \"*\" * 60)\n",
        "    args = parser.parse_args()  \n",
        "    main(args)\n",
        "    print(\"*\" * 60 + \"\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/batch_endpoint.py\n"
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the components\n",
        "\n",
        "To define the component you need to specify:\n",
        "\n",
        "- **Metadata**: *name*, *display name*, *version*, *description*, *type* etc. The metadata helps to describe and manage the component.\n",
        "- **Interface**: *inputs* and *outputs*. For example, a model training component will take training data and the regularization rate as input, and generate a trained model file as output. \n",
        "- **Command, code & environment**: the *command*, *code* and *environment* to run the component. Command is the shell command to execute the component. Code usually refers to a source code directory. Environment could be an AzureML environment (curated or custom created), docker image or conda environment.\n",
        "\n",
        "Run the following cells to create a YAML for each component you want to run as a pipeline step."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/accident-prediction-env.yml\n",
        "name: accident-prediction-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - pip\n",
        "  - pip:\n",
        "      - mlflow\n",
        "      - pandas\n",
        "      - scikit-learn\n",
        "      - numpy\n",
        "      - azureml-core  \n",
        "      - azureml-defaults\n",
        "      - fsspec \n",
        "      - azure-identity\n",
        "      - azure-ai-ml"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/accident-prediction-env.yml\n"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "# Create environment with Conda and Docker\n",
        "environment_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    conda_file=\"src/accident-prediction-env.yml\",  # Ensure this file exists\n",
        "    name=\"accident-prediction-environment\",\n",
        "    description=\"Environment for accident prediction batch deployment\",\n",
        "\n",
        ")\n",
        "\n",
        "# Register the environment\n",
        "registered_env = ml_client.environments.create_or_update(environment_conda)\n",
        "print(f\" Environment registered: {registered_env.name}, Version: {registered_env.version}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " Environment registered: accident-prediction-environment, Version: 1\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1740323189813
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/transform_data.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: transform_data\n",
        "display_name: transform Data in Azure ML\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  input_data: \n",
        "    type: uri_folder\n",
        "    description: Directory containing multiple CSV files\n",
        "  feature_transformer_name:\n",
        "    type: string\n",
        "    description: Name of the feature transformer to use\n",
        "outputs:\n",
        "   output_data:\n",
        "     type: uri_folder\n",
        "     description: output the transformed data here for the batch prediction endpoint\n",
        "code: .\n",
        "environment: azureml:accident-prediction-environment:1\n",
        "command: >-\n",
        "  python transform_data.py \n",
        "  --input_data ${{inputs.input_data}}\n",
        "  --feature_transformer_name ${{inputs.feature_transformer_name}}\n",
        "  --output_data ${{outputs.output_data}}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/transform_data.yml\n"
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/batch_prediction.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: batch_prediction\n",
        "display_name: Batch Prediction\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  training_data:\n",
        "    type: uri_folder\n",
        "    description: Input data for batch prediction\n",
        "  model_name:\n",
        "    type: string\n",
        "    default: accident-survival-model\n",
        "    description: Name of the model to use for prediction\n",
        "outputs:\n",
        "  predictions:\n",
        "    type: uri_folder\n",
        "    description: Output directory for predictions\n",
        "code: .\n",
        "environment: azureml:accident-prediction-environment@latest\n",
        "command: >-\n",
        "  python batch_scoring.py\n",
        "  --training_data_path ${{inputs.training_data}}\n",
        "  --model_name ${{inputs.model_name}}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/batch_prediction.yml\n"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import load_component, dsl, Input\n",
        "from azure.ai.ml.entities import Pipeline\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "@dsl.pipeline(\n",
        "    description=\"End-to-end accident classification pipeline\",\n",
        "    default_compute=\"captgt0071\"\n",
        ")\n",
        "def accident_classification(\n",
        "    pipeline_job_input: Input(type=AssetTypes.URI_FOLDER)\n",
        "):\n",
        "    # Load components with explicit paths\n",
        "    prep_data = load_component(source=\"./src/transform_data.yml\")\n",
        "    batch_work = load_component(source=\"./src/batch_prediction.yml\")\n",
        "    \n",
        "    clean_data = prep_data(\n",
        "        input_data=pipeline_job_input,\n",
        "        feature_transformer_name=\"accident-transformer\"\n",
        "    )\n",
        "    \n",
        "    # Make predictions using batch endpoint\n",
        "    predictions = batch_work(\n",
        "        training_data=clean_data.outputs.output_data,\n",
        "        model_name=\"accident-survival-model\"\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"transformed_batch_data\": clean_data.outputs.output_data,\n",
        "        \"computed_predictions\": predictions.outputs.predictions\n",
        "    }\n",
        "\n",
        "def submit_pipeline(ml_client):\n",
        "    try:\n",
        "        # Create pipeline job\n",
        "        pipeline_job = accident_classification(\n",
        "            pipeline_job_input=Input(type=AssetTypes.URI_FOLDER, path=\"./data\")\n",
        "        )\n",
        "        print(pipeline_job)\n",
        "        \n",
        "        # Configure pipeline settings\n",
        "        pipeline_job.settings.default_compute = \"captgt0071\"\n",
        "        pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
        "        \n",
        "        # Set output modes\n",
        "        pipeline_job.outputs.transformed_batch_data.mode = \"upload\"\n",
        "        pipeline_job.outputs.computed_predictions.mode = \"upload\"\n",
        "        \n",
        "        # Submit the pipeline\n",
        "        submitted_pipeline = ml_client.jobs.create_or_update(\n",
        "            pipeline_job,\n",
        "            experiment_name=\"pipeline_accident_survival_prediction\"\n",
        "        )\n",
        "        \n",
        "        return submitted_pipeline\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error submitting pipeline: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize ML client\n",
        "    # Get Azure ML client\n",
        "    try:\n",
        "        # Try DefaultAzureCredential first\n",
        "        credential = DefaultAzureCredential()\n",
        "        ml_client = MLClient(\n",
        "            credential=credential,\n",
        "            subscription_id=\"cda9116f-5326-4a9b-9407-bc3a4391c27c\",\n",
        "            resource_group_name=\"gabby102\",\n",
        "            workspace_name=\"health-update\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Default credential failed: {e}\")\n",
        "        print(\"Falling back to interactive browser login...\")\n",
        "        try:\n",
        "            # Fall back to interactive login\n",
        "            credential = InteractiveBrowserCredential()\n",
        "            ml_client = MLClient(\n",
        "                credential=credential,\n",
        "                subscription_id=\"cda9116f-5326-4a9b-9407-bc3a4391c27c\",\n",
        "                resource_group_name=\"gabby102\",\n",
        "                workspace_name=\"health-update\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Interactive login failed: {e}\")\n",
        "        \n",
        "\n",
        "    \n",
        "    # Submit job to workspace\n",
        "    try:\n",
        "        pipeline_job = submit_pipeline(ml_client)\n",
        "        print(f\"Pipeline submitted successfully. Job ID: {pipeline_job.id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to submit pipeline: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Overriding of current TracerProvider is not allowed\nOverriding of current LoggerProvider is not allowed\nOverriding of current MeterProvider is not allowed\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\n\u001b[32mUploading src (0.02 MBs): 100%|██████████| 22867/22867 [00:00<00:00, 689616.33it/s]\n\u001b[39m\n\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "display_name: accident_classification\ndescription: End-to-end accident classification pipeline\ntype: pipeline\ninputs:\n  pipeline_job_input:\n    type: uri_folder\n    path: azureml:./data\noutputs:\n  transformed_batch_data:\n    type: uri_folder\n  computed_predictions:\n    type: uri_folder\njobs:\n  clean_data:\n    type: command\n    inputs:\n      input_data:\n        path: ${{parent.inputs.pipeline_job_input}}\n      feature_transformer_name: accident-transformer\n    outputs:\n      output_data: ${{parent.outputs.transformed_batch_data}}\n    component:\n      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n      name: transform_data\n      version: '1'\n      display_name: transform Data in Azure ML\n      type: command\n      inputs:\n        input_data:\n          type: uri_folder\n          description: Directory containing multiple CSV files\n        feature_transformer_name:\n          type: string\n          description: Name of the feature transformer to use\n      outputs:\n        output_data:\n          type: uri_folder\n          description: output the transformed data here for the batch prediction endpoint\n      command: python transform_data.py  --input_data ${{inputs.input_data}} --feature_transformer_name\n        ${{inputs.feature_transformer_name}} --output_data ${{outputs.output_data}}\n      environment: azureml:accident-prediction-environment:1\n      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11/src\n      is_deterministic: true\n  predictions:\n    type: command\n    inputs:\n      training_data:\n        path: ${{parent.jobs.clean_data.outputs.output_data}}\n      model_name: accident-survival-model\n    outputs:\n      predictions: ${{parent.outputs.computed_predictions}}\n    component:\n      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n      name: batch_prediction\n      version: '1'\n      display_name: Batch Prediction\n      type: command\n      inputs:\n        training_data:\n          type: uri_folder\n          description: Input data for batch prediction\n        model_name:\n          type: string\n          default: accident-survival-model\n          description: Name of the model to use for prediction\n      outputs:\n        predictions:\n          type: uri_folder\n          description: Output directory for predictions\n      command: python batch_scoring.py --training_data_path ${{inputs.training_data}}\n        --model_name ${{inputs.model_name}}\n      environment: azureml:accident-prediction-environment@latest\n      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11/src\n      is_deterministic: true\nsettings:\n  default_compute: azureml:captgt0071\n\nPipeline submitted successfully. Job ID: /subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/gabby102/providers/Microsoft.MachineLearningServices/workspaces/health-update/jobs/plucky_fowl_9zxf0dznl9\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1740324190402
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ml_client.jobs.get('plucky_fowl_9zxf0dznl9')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f1407070f10>}, 'outputs': {'transformed_batch_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f1407071120>, 'computed_predictions': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f1407070af0>}, 'jobs': {}, 'component': PipelineComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': 'End-to-end accident classification pipeline', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f1407070bb0>, 'version': '1', 'schema': None, 'type': 'pipeline', 'display_name': 'accident_classification', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'transformed_batch_data': {}, 'computed_predictions': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'clean_data': Command({'parameters': {}, 'init': False, 'name': 'clean_data', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f141f8635e0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'feature_transformer_name': 'accident-transformer', 'input_data': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_data': '${{parent.outputs.transformed_batch_data}}'}, 'inputs': {'feature_transformer_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f1407070c70>, 'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f1407070dc0>}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f1407db7a60>}, 'component': 'azureml_anonymous:f04bb6b5-fb1b-465b-8a14-2485b1c23639', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '37412010-6a7f-4e1e-a222-ddcd0459a1b3', 'source': 'YAML.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'predictions': Command({'parameters': {}, 'init': False, 'name': 'predictions', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f1407070640>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'model_name': 'accident-survival-model', 'training_data': '${{parent.jobs.clean_data.outputs.output_data}}'}, 'job_outputs': {'predictions': '${{parent.outputs.computed_predictions}}'}, 'inputs': {'model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f1407073ca0>, 'training_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f1407070f70>}, 'outputs': {'predictions': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f14070709d0>}, 'component': 'azureml_anonymous:a2f7b6c2-52d7-4135-b62a-2ea0fcb4442f', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '2c0fdb5e-1eed-4e5a-b99c-4962267e5dac', 'source': 'YAML.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'YAML.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Completed', 'log_files': None, 'name': 'plucky_fowl_9zxf0dznl9', 'description': 'End-to-end accident classification pipeline', 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'captgt0071', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun', 'azureml.pipelines.stages': '{\"Initialization\":null,\"Execution\":{\"StartTime\":\"2025-02-23T15:23:13.2777633+00:00\",\"EndTime\":\"2025-02-23T15:24:10.4022541+00:00\",\"Status\":\"Finished\"}}'}, 'print_as_yaml': False, 'id': '/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/gabby102/providers/Microsoft.MachineLearningServices/workspaces/health-update/jobs/plucky_fowl_9zxf0dznl9', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/11', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f1407070910>, 'serialize': <msrest.serialization.Serializer object at 0x7f1407070580>, 'display_name': 'accident_classification', 'experiment_name': 'pipeline_accident_survival_prediction', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/gabby102/providers/Microsoft.MachineLearningServices/workspaces/health-update?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/plucky_fowl_9zxf0dznl9?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/gabby102/workspaces/health-update&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_accident_survival_prediction</td><td>plucky_fowl_9zxf0dznl9</td><td>pipeline</td><td>Completed</td><td><a href=\"https://ml.azure.com/runs/plucky_fowl_9zxf0dznl9?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/gabby102/workspaces/health-update&amp;tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1740324573267
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Get the results\n",
        "\n",
        "When the pipeline job that invokes the batch endpoint is completed, you can view the results. All predictions are collected in the `predictions.csv` file that is stored in the default datastore. You can download the file and visualize the data by running the following cells. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.download(name='plucky_fowl_9zxf0dznl9', download_path=\".\", output_name=\"score\")"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1740324610371
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"predictions.csv\", \"r\") as f:\n",
        "    data = f.read()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'predictions.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predictions.csv'"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1740324613440
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "import pandas as pd\n",
        "\n",
        "score = pd.DataFrame(\n",
        "    literal_eval(data.replace(\"\\n\", \",\")), columns=[\"file\", \"prediction\"]\n",
        ")\n",
        "score"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1667817550830
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "python"
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}