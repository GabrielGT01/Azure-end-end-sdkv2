{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Log models with MLflow\n",
        "\n",
        "You can use MLflow in Azure Machine Learning to log models. When you log a model as a model instead of an artifact, a MLmodel is created in the output directory. The MLmodel file contains all the model's metadata. You can customize the model's signature when logging the model.\n",
        "\n",
        "## Before you start\n",
        "\n",
        "You'll need the latest version of the **azure-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
        "\n",
        "> **Note**:\n",
        "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1739454029443
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: azure-ai-ml\r\n",
            "Version: 1.25.0\r\n",
            "Summary: Microsoft Azure Machine Learning Client Library for Python\r\n",
            "Home-page: https://github.com/Azure/azure-sdk-for-python\r\n",
            "Author: Microsoft Corporation\r\n",
            "Author-email: azuresdkengsysadmins@microsoft.com\r\n",
            "License: MIT License\r\n",
            "Location: /anaconda/envs/azureml_py38/lib/python3.10/site-packages\r\n",
            "Requires: azure-common, azure-core, azure-mgmt-core, azure-monitor-opentelemetry, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, msrest, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\r\n",
            "Required-by: \r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip show azure-ai-ml"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1739454105215
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1739454114625
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        }
      ],
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autologging with MLflow\n",
        "\n",
        "When you use autologging, your model is automatically logged. The model flavor and schema is inferred. \n",
        "\n",
        "Run the following cell to create the **train-model-autolog.py** script in the **src** folder. The script trains a classification model by using the **accident.csv** file in the same folder, which is passed as an argument. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1739454118884
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src folder created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/train-model-autolog.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/train-model-autolog.py\n",
        "# import libraries\n",
        "import mlflow\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "def main(args):\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    # train model\n",
        "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    eval_model(model, X_test, y_test)\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    print(\"Reading data...\")\n",
        "    data = pd.read_csv(path)\n",
        "    df = data.copy().dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# function that splits the data\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    # Numeric transformer pipeline\n",
        "    numeric_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"mean\"),\n",
        "        StandardScaler(),\n",
        "    )\n",
        "    \n",
        "    # Categorical transformer pipeline\n",
        "    categorical_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(drop='first')\n",
        "    )\n",
        "    \n",
        "    # Define categorical and numeric columns\n",
        "    cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    num_columns = ['Age', 'Speed_of_Impact']\n",
        "    \n",
        "    # Combined feature transformer\n",
        "    features_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"numeric\", numeric_transformer, num_columns),\n",
        "            (\"categorical\", categorical_transformer, cat_columns),\n",
        "        ],\n",
        "    )\n",
        "    # Separate features and labels\n",
        "    X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "    y = df['Survived'].values\n",
        "\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    \n",
        "    # Transform train and test data\n",
        "    X_train = features_transformer.fit_transform(X_train)\n",
        "    X_test = features_transformer.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# function that trains the model\n",
        "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
        "    print(\"Training model...\")\n",
        "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "# function that evaluates the model\n",
        "def eval_model(model, X_test, y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' + str(auc))\n",
        "\n",
        "    # plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    # Plot the diagonal 50% line\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # Plot the FPR and TPR achieved by our model\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.savefig(\"ROC-Curve.png\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
        "                        type=float, default=0.01)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you can submit the script as a command job.\n",
        "\n",
        "Run the cell below to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1739448541450
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "\u001b[32mUploading src (0.45 MBs): 100%|██████████| 446586/446586 [00:00<00:00, 14314888.28it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monitor your job at https://ml.azure.com/runs/quirky_quince_1rwgzl654p?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/dala_project&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python train-model-autolog.py --training_data accident.csv\",\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"captgt0071\",\n",
        "    display_name=\"accident-train-autolog\",\n",
        "    experiment_name=\"accident-training\"\n",
        "    )\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Studio, navigate to the **diabetes-train-autolog** job to explore the overview of the command job you ran. Find the logged artifacts in the **Outputs + logs** tab. Select the `model` folder to find the `MLmodel` file and explore its contents."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify the flavor with autologging\n",
        "\n",
        "You can use autologging, but still specify the flavor of the model. In the example, the model's flavor is scikit-learn.so use sklearn auto logging\n",
        "\n",
        "Run the following cell to create the **train-model-sklearn.py** script in the **src** folder. The script trains a classification model by using the **accident.csv** file in the same folder, which is passed as an argument. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/train-model-sklearn.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/train-model-sklearn.py\n",
        "# import libraries\n",
        "import mlflow\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "def main(args):\n",
        "    # enable autologging\n",
        "    mlflow.sklearn.autolog()\n",
        "\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    # train model\n",
        "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    eval_model(model, X_test, y_test)\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    print(\"Reading data...\")\n",
        "    data = pd.read_csv(path)\n",
        "    df = data.copy().dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# function that splits the data\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    # Numeric transformer pipeline\n",
        "    numeric_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"mean\"),\n",
        "        StandardScaler(),\n",
        "    )\n",
        "    \n",
        "    # Categorical transformer pipeline\n",
        "    categorical_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(drop='first')\n",
        "    )\n",
        "    \n",
        "    # Define categorical and numeric columns\n",
        "    cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    num_columns = ['Age', 'Speed_of_Impact']\n",
        "    \n",
        "    # Combined feature transformer\n",
        "    features_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"numeric\", numeric_transformer, num_columns),\n",
        "            (\"categorical\", categorical_transformer, cat_columns),\n",
        "        ],\n",
        "    )\n",
        "    # Separate features and labels\n",
        "    X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "    y = df['Survived'].values\n",
        "\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    \n",
        "    # Transform train and test data\n",
        "    X_train = features_transformer.fit_transform(X_train)\n",
        "    X_test = features_transformer.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# function that trains the model\n",
        "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
        "    print(\"Training model...\")\n",
        "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "# function that evaluates the model\n",
        "def eval_model(model, X_test, y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' + str(auc))\n",
        "\n",
        "    # plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    # Plot the diagonal 50% line\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # Plot the FPR and TPR achieved by our model\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.savefig(\"ROC-Curve.png\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
        "                        type=float, default=0.01)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you can submit the script as a command job.\n",
        "\n",
        "Run the cell below to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1739449816633
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r\u001b[32mUploading src (0.45 MBs):   0%|          | 0/450581 [00:00<?, ?it/s]\r\u001b[32mUploading src (0.45 MBs): 100%|██████████| 450581/450581 [00:00<00:00, 15924381.02it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monitor your job at https://ml.azure.com/runs/calm_book_hz88rfymdw?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/dala_project&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python train-model-sklearn.py --training_data accident.csv\",\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"captgt0071\",\n",
        "    display_name=\"accident-train-sklearn\",\n",
        "    experiment_name=\"accident-training\"\n",
        "    )\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Studio, navigate to the **accident-train-sklearn** job to explore the overview of the command job you ran. Find the logged artifacts in the **Outputs + logs** tab. Select the `model` folder to find the `MLmodel` file and explore its contents.\n",
        "\n",
        "Compare the `MLmodel` files of the previous two runs. You'll notice that they're the same, indicating that MLflow's autolog feature correctly inferred the model's flavor."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customize the model with an inferred signature\n",
        "\n",
        "You can manually log the model using `mlflow.sklearn.log_model` instead of autologging. You'll create a signature by inferring it from the training dataset and predicted results. And finally, you'll log the scikit-learn model.\n",
        "\n",
        "Run the following cell to create the **train-model-infer.py** script in the **src** folder. The script trains a classification model by using the **diabetes.csv** file in the same folder, which is passed as an argument. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing src/train-model-infer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/train-model-infer.py\n",
        "# import libraries\n",
        "import mlflow\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Define paths for saving model and transformer\n",
        "MODEL_PATH = \"model.pkl\"\n",
        "TRANSFORMER_PATH = \"features_transformer.pkl\"\n",
        "\n",
        "def main(args):\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test, features_transformer = split_data(df)\n",
        "\n",
        "    # Save the feature transformer for future use\n",
        "    joblib.dump(features_transformer, TRANSFORMER_PATH)\n",
        "    print(f\"Feature transformer saved at: {TRANSFORMER_PATH}\")\n",
        "\n",
        "    # train model\n",
        "    model = train_model(args.reg_rate, X_train, y_train)\n",
        "    mlflow.log_param(\"regularization_rate\", args.reg_rate)\n",
        "\n",
        "    # Save trained model\n",
        "    joblib.dump(model, MODEL_PATH)\n",
        "    print(f\"Trained model saved at: {MODEL_PATH}\")\n",
        "\n",
        "    # evaluate model\n",
        "    y_hat = eval_model(model, X_test, y_test)\n",
        "\n",
        "    # create the signature by inferring it from the datasets\n",
        "    signature = infer_signature(X_test, y_hat)\n",
        "\n",
        "    # Log the model and feature transformer in MLflow\n",
        "    mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n",
        "    mlflow.log_artifact(TRANSFORMER_PATH, artifact_path=\"preprocessor\")\n",
        "\n",
        "    print(\"Model and Feature Transformer saved in MLflow!\")\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    print(\"Reading data...\")\n",
        "    data = pd.read_csv(path)\n",
        "    df = data.copy().dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# function that splits the data\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    \n",
        "    # Numeric transformer pipeline\n",
        "    numeric_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"mean\"),\n",
        "        StandardScaler(),\n",
        "    )\n",
        "    \n",
        "    # Categorical transformer pipeline\n",
        "    categorical_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(drop='first')\n",
        "    )\n",
        "    \n",
        "    # Define categorical and numeric columns\n",
        "    cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    num_columns = ['Age', 'Speed_of_Impact']\n",
        "    \n",
        "    # Combined feature transformer\n",
        "    features_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"numeric\", numeric_transformer, num_columns),\n",
        "            (\"categorical\", categorical_transformer, cat_columns),\n",
        "        ],\n",
        "    )\n",
        "    \n",
        "    # Separate features and labels\n",
        "    X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "    y = df['Survived'].values\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    \n",
        "    # Transform train and test data\n",
        "    X_train = features_transformer.fit_transform(X_train)\n",
        "    X_test = features_transformer.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, features_transformer\n",
        "\n",
        "# function that trains the model\n",
        "def train_model(reg_rate, X_train, y_train):\n",
        "    print(\"Training model...\")\n",
        "\n",
        "    # Train logistic regression model\n",
        "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "# function that evaluates the model\n",
        "def eval_model(model, X_test, y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test, y_scores[:,1])\n",
        "    print('AUC:', auc)\n",
        "\n",
        "    mlflow.log_metric(\"accuracy\", acc)\n",
        "    mlflow.log_metric(\"auc\", auc)\n",
        "\n",
        "    # plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line\n",
        "    plt.plot(fpr, tpr)  # ROC curve\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.savefig(\"ROC-Curve.png\")\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data', type=str, required=True)\n",
        "    parser.add_argument(\"--reg_rate\", dest='reg_rate', type=float, default=0.01,help=\"Regularization rate must be positive\")\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you can submit the script as a command job.\n",
        "\n",
        "Run the cell below to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1739454199610
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "\u001b[32mUploading src (0.46 MBs): 100%|██████████| 455406/455406 [00:00<00:00, 16053378.22it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monitor your job at https://ml.azure.com/runs/wheat_camera_mpd9f1rc74?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/dala_project&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python train-model-infer.py --training_data accident.csv\",\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"captgt0071\",\n",
        "    display_name=\"accident-train-infer\",\n",
        "    experiment_name=\"accident-training\"\n",
        "    )\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Studio, navigate to the **accident-train-infer** job to explore the overview of the command job you ran. Find the logged artifacts in the **Outputs + logs** tab. Select the `model` folder to find the `MLmodel` file and explore its contents.\n",
        "\n",
        "Compare the `MLmodel` files with the previous two runs. You'll notice that they're all the same, indicating that MLflow's autolog feature correctly inferred the model's signature too."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customize the model with a defined signature\n",
        "\n",
        "You can manually log the model using `mlflow.sklearn.log_model`. You'll also create a signature manually. And finally, you'll log the scikit-learn model.\n",
        "\n",
        "Run the following cell to create the **train-model-signature.py** script in the **src** folder. The script trains a classification model by using the **accident.csv** file in the same folder, which is passed as an argument. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing src/train-model-signature.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/train-model-signature.py\n",
        "# import libraries\n",
        "import mlflow\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from mlflow.models import infer_signature\n",
        "import mlflow.sklearn\n",
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types.schema import Schema, ColSpec\n",
        "\n",
        "# Define paths for saving model and transformer\n",
        "MODEL_PATH = \"model.pkl\"\n",
        "TRANSFORMER_PATH = \"features_transformer.pkl\"\n",
        "\n",
        "def main(args):\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test, features_transformer = split_data(df)\n",
        "\n",
        "    # Save the feature transformer for future use\n",
        "    joblib.dump(features_transformer, TRANSFORMER_PATH)\n",
        "    print(f\"Feature transformer saved at: {TRANSFORMER_PATH}\")\n",
        "\n",
        "    # train model\n",
        "    model = train_model(args.reg_rate, X_train, y_train)\n",
        "    mlflow.log_param(\"regularization_rate\", args.reg_rate)\n",
        "\n",
        "    # Save trained model\n",
        "    joblib.dump(model, MODEL_PATH)\n",
        "    print(f\"Trained model saved at: {MODEL_PATH}\")\n",
        "\n",
        "    # evaluate model\n",
        "    y_hat = eval_model(model, X_test, y_test)\n",
        "\n",
        "    # create the signature manually\n",
        "    \n",
        "    input_schema = Schema([\n",
        "        ColSpec(\"double\", \"Age\"),  # Assuming age is a float/double\n",
        "        ColSpec(\"string\", \"Gender\"),  # Categorical variable\n",
        "        ColSpec(\"double\", \"Speed_of_Impact\"),  # Assuming speed is a float/double\n",
        "        ColSpec(\"string\", \"Helmet_Used\"),  # Categorical variable\n",
        "        ColSpec(\"string\", \"Seatbelt_Used\")  # Categorical variable\n",
        "    ])\n",
        "\n",
        "    output_schema = Schema([ColSpec(\"integer\", \"Survived\")])\n",
        "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
        " \n",
        "\n",
        "    # Log the model and feature transformer in MLflow\n",
        "    mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature)\n",
        "    mlflow.log_artifact(TRANSFORMER_PATH, artifact_path=\"preprocessor\")\n",
        "\n",
        "    print(\"Model and Feature Transformer saved in MLflow!\")\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    print(\"Reading data...\")\n",
        "    data = pd.read_csv(path)\n",
        "    # Convert numeric columns to float\n",
        "    data['Age'] = data['Age'].astype(float)\n",
        "    data['Speed_of_Impact'] = data['Speed_of_Impact'].astype(float)\n",
        "    df = data.copy().dropna()\n",
        "    return df\n",
        "\n",
        "# function that splits the data\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    \n",
        "    # Numeric transformer pipeline\n",
        "    numeric_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"mean\"),\n",
        "        StandardScaler(),\n",
        "    )\n",
        "    \n",
        "    # Categorical transformer pipeline\n",
        "    categorical_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(drop='first')\n",
        "    )\n",
        "    \n",
        "    # Define categorical and numeric columns\n",
        "    cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    num_columns = ['Age', 'Speed_of_Impact']\n",
        "    \n",
        "    # Combined feature transformer\n",
        "    features_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"numeric\", numeric_transformer, num_columns),\n",
        "            (\"categorical\", categorical_transformer, cat_columns),\n",
        "        ],\n",
        "    )\n",
        "    \n",
        "    # Separate features and labels\n",
        "    X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "    y = df['Survived'].values\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    \n",
        "    # Transform train and test data\n",
        "    X_train = features_transformer.fit_transform(X_train)\n",
        "    X_test = features_transformer.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, features_transformer\n",
        "\n",
        "# function that trains the model\n",
        "def train_model(reg_rate, X_train, y_train):\n",
        "    print(\"Training model...\")\n",
        "\n",
        "    # Train logistic regression model\n",
        "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "# function that evaluates the model\n",
        "def eval_model(model, X_test, y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test, y_scores[:,1])\n",
        "    print('AUC:', auc)\n",
        "\n",
        "    mlflow.log_metric(\"accuracy\", acc)\n",
        "    mlflow.log_metric(\"auc\", auc)\n",
        "\n",
        "    # plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal reference line\n",
        "    plt.plot(fpr, tpr)  # ROC curve\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.savefig(\"ROC-Curve.png\")\n",
        "    plt.savefig(\"ROC-Curve.png\")\n",
        "    mlflow.log_artifact(\"ROC-Curve.png\")\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data', type=str, required=True)\n",
        "    parser.add_argument(\"--reg_rate\", dest='reg_rate', type=float, default=0.01,help=\"Regularization rate must be positive\")\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you can submit the script as a command job.\n",
        "\n",
        "Run the cell below to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1739455131786
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading src (0.46 MBs): 100%|██████████| 461023/461023 [00:00<00:00, 11497078.35it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monitor your job at https://ml.azure.com/runs/quirky_candle_jky5bkyprc?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/dala_project&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python train-model-signature.py --training_data accident.csv\",\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"captgt0071\",\n",
        "    display_name=\"accident-train-signature\",\n",
        "    experiment_name=\"accident-training\"\n",
        "    )\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Studio, navigate to the **accident-train-signature** job to explore the overview of the command job you ran. Find the logged artifacts in the **Outputs + logs** tab. Select the `model` folder to find the `MLmodel` file and explore its contents.\n",
        "\n",
        "Compare the `MLmodel` files with the previous runs. You'll notice that the signature is different from the previous runs. Previous runs used tensor-based signatures, whereas the latest run used a column-based signature."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the model\n",
        "\n",
        "When you choose a model you want to deploy, you can first register the model. \n",
        "\n",
        "To register the latest model, you'll refer to the name of the job run. By registering the model as an MLflow model, you can easily deploy it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1739455460402
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>accident-training</td><td>quirky_candle_jky5bkyprc</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/quirky_candle_jky5bkyprc?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/dala_project&amp;tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
            ],
            "text/plain": [
              "Command({'parameters': {}, 'init': False, 'name': 'quirky_candle_jky5bkyprc', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'_azureml.ComputeTargetType': 'amlcdsi', '_azureml.ClusterName': 'captgt0071', 'ContentSnapshotId': 'c6c400f1-b0ae-43b8-9e3d-8d5eee7aaac1'}, 'print_as_yaml': False, 'id': '/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/mlw-dp100/providers/Microsoft.MachineLearningServices/workspaces/dala_project/jobs/quirky_candle_jky5bkyprc', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/10', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f345ddd64d0>, 'serialize': <msrest.serialization.Serializer object at 0x7f345ddd4340>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'accident-train-signature', 'experiment_name': 'accident-training', 'compute': 'captgt0071', 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/mlw-dp100/providers/Microsoft.MachineLearningServices/workspaces/dala_project?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/quirky_candle_jky5bkyprc?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/dala_project&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.quirky_candle_jky5bkyprc', 'mode': 'rw_mount'}}, 'inputs': {}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f345ddd4370>}, 'component': CommandComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'quirky_candle_jky5bkyprc', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/10', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f345ddd64d0>, 'serialize': <msrest.serialization.Serializer object at 0x7f345ddd63e0>, 'command': 'python train-model-signature.py --training_data accident.csv', 'code': '/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/mlw-dp100/providers/Microsoft.MachineLearningServices/workspaces/dala_project/codes/9b3baabc-3079-43f3-9f03-2d5537193365/versions/1', 'environment_variables': {}, 'environment': 'azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'schema': None, 'type': 'command', 'display_name': 'accident-train-signature', 'is_deterministic': True, 'inputs': {}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.quirky_candle_jky5bkyprc', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': []}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/mlw-dp100/providers/Microsoft.MachineLearningServices/workspaces/dala_project?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/quirky_candle_jky5bkyprc?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/dala_project&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f345ddd64d0>}, 'instance_id': '5e8e31b7-8398-4dd8-b842-70a1275f2c47', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "returned_job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1739455455666
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'quirky_candle_jky5bkyprc'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "job_name = returned_job.name\n",
        "\n",
        "job_name "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1739455537653
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'azureml://jobs/quirky_candle_jky5bkyprc/outputs/artifacts/paths/model/'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f\"azureml://jobs/{job_name}/outputs/artifacts/paths/model/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "job_name = returned_job.name\n",
        "\n",
        "run_model = Model(\n",
        "    path=f\"azureml://jobs/{job_name}/outputs/artifacts/paths/model/\",\n",
        "    name=\"mlflow-accident\",\n",
        "    description=\"Model created from run.\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        ")\n",
        "# Uncomment after adding required details above\n",
        "ml_client.models.create_or_update(run_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Studio, navigate to the **Models** page. In the model list, find the `mlflow-diabetes` model and select it to explore it further.\n",
        "\n",
        "- In the **Details** tab of the `mlflow-diabetes` model, you can review that it's a `MLFLOW` type model and the job that trained the model.\n",
        "- In the **Artifacts** tab you can find the directory with the `MLmodel` file.\n",
        "\n",
        "If you want to explore the model's behavior further, you can **optionally** choose to deploy the model to a real-time endpoint."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
