{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Run a training script as a command job\n",
        "\n",
        "You can use the Python SDK for Azure Machine Learning to submit scripts as command jobs. By using jobs, you can easily keep track of the input parameters and outputs when training a machine learning model.\n",
        "\n",
        "## Before you start\n",
        "\n",
        "You'll need the latest version of the **azure-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
        "\n",
        "> **Note**:\n",
        "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-ai-ml"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: azure-ai-ml in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.24.0)\r\nRequirement already satisfied: azure-storage-file-datalake>=12.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (12.18.1)\r\nRequirement already satisfied: azure-core>=1.23.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.30.2)\r\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.66.4)\r\nRequirement already satisfied: strictyaml in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.7.3)\r\nRequirement already satisfied: jsonschema>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.23.0)\r\nRequirement already satisfied: pyjwt in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (2.4.0)\r\nRequirement already satisfied: colorama in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.4.6)\r\nRequirement already satisfied: azure-common>=1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.1.28)\r\nRequirement already satisfied: msrest>=0.6.18 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.7.1)\r\nRequirement already satisfied: azure-storage-blob>=12.10.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (12.24.1)\r\nRequirement already satisfied: azure-monitor-opentelemetry in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.6.4)\r\nRequirement already satisfied: isodate in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.6.1)\r\nRequirement already satisfied: pydash>=6.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (8.0.5)\r\nRequirement already satisfied: pyyaml>=5.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (6.0.1)\r\nRequirement already satisfied: azure-storage-file-share in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (12.20.1)\r\nRequirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.12.2)\r\nRequirement already satisfied: azure-mgmt-core>=1.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.4.0)\r\nRequirement already satisfied: marshmallow>=3.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (3.26.1)\r\nRequirement already satisfied: requests>=2.21.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-ml) (2.32.3)\r\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-ml) (1.16.0)\r\nRequirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-storage-blob>=12.10.0->azure-ai-ml) (38.0.4)\r\nRequirement already satisfied: attrs>=22.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (24.2.0)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (2023.12.1)\r\nRequirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.19.1)\r\nRequirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.35.1)\r\nRequirement already satisfied: packaging>=17.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from marshmallow>=3.5->azure-ai-ml) (24.1)\r\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest>=0.6.18->azure-ai-ml) (2.0.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest>=0.6.18->azure-ai-ml) (2024.8.30)\r\nRequirement already satisfied: azure-core-tracing-opentelemetry~=1.0.0b11 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.0.0b11)\r\nRequirement already satisfied: opentelemetry-instrumentation-fastapi~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-instrumentation-requests~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-instrumentation-urllib3~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-resource-detector-azure~=0.1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.1.5)\r\nRequirement already satisfied: azure-monitor-opentelemetry-exporter~=1.0.0b31 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.0.0b33)\r\nRequirement already satisfied: opentelemetry-instrumentation-urllib~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-instrumentation-django~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-sdk~=1.28 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.30.0)\r\nRequirement already satisfied: opentelemetry-instrumentation-psycopg2~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-instrumentation-flask~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: python-dateutil>=2.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from strictyaml->azure-ai-ml) (2.9.0.post0)\r\nRequirement already satisfied: opentelemetry-api<2.0.0,>=1.12.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (1.30.0)\r\nRequirement already satisfied: fixedint==0.1.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b31->azure-monitor-opentelemetry->azure-ai-ml) (0.1.6)\r\nRequirement already satisfied: psutil~=5.9 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b31->azure-monitor-opentelemetry->azure-ai-ml) (5.9.3)\r\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (1.16.0)\r\nRequirement already satisfied: opentelemetry-instrumentation-wsgi==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-instrumentation==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: opentelemetry-util-http==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (1.14.1)\r\nRequirement already satisfied: deprecated>=1.2.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-semantic-conventions==0.51b0->opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (1.2.14)\r\nRequirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (8.2.0)\r\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: asgiref~=3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (3.8.1)\r\nRequirement already satisfied: opentelemetry-instrumentation-dbapi==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-psycopg2~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\r\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.3.2)\r\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.7)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (1.26.19)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.2)\r\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (2.22)\r\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api<2.0.0,>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (3.19.2)\r\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip show azure-ai-ml"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: azure-ai-ml\r\nVersion: 1.24.0\r\nSummary: Microsoft Azure Machine Learning Client Library for Python\r\nHome-page: https://github.com/Azure/azure-sdk-for-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: azuresdkengsysadmins@microsoft.com\r\nLicense: MIT License\r\nLocation: /anaconda/envs/azureml_py38/lib/python3.10/site-packages\r\nRequires: azure-common, azure-core, azure-mgmt-core, azure-monitor-opentelemetry, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, msrest, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\r\nRequired-by: \r\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1739285523767
        },
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1739285534903
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1739285536721
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Custom tracking with MLflow\n",
        "\n",
        "When running a script as a job you can use MLflow in your training script to track the model. MLflow allows you to track any custom parameters, metrics, or artifacts you want to store with your job output.\n",
        "\n",
        "Run the following cells to create the **train-model-mlflow.py** script in the **src** folder. The script trains a classification model by using the **accident.csv** file in the same folder, which is passed as an argument. \n",
        "\n",
        "Review the code below to find that the script will import `mlflow` and log:\n",
        "\n",
        "- The regularization rate as a **parameter**. \n",
        "- The accuracy and AUC as **metrics**.\n",
        "- The plotted ROC curve as an **artifact**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "src folder created\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1739285649811
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/mlflow-accident-training-model-parameters.py\n",
        "# import libraries\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "def main(args):\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    # train model\n",
        "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # evaluate model\n",
        "    eval_model(model, X_test, y_test)\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    print(\"Reading data...\")\n",
        "    data = pd.read_csv(path)\n",
        "    df = data.copy().dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# function that splits the data\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    # Numeric transformer pipeline\n",
        "    numeric_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"mean\"),\n",
        "        StandardScaler(),\n",
        "    )\n",
        "    \n",
        "    # Categorical transformer pipeline\n",
        "    categorical_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(drop='first')\n",
        "    )\n",
        "    \n",
        "    # Define categorical and numeric columns\n",
        "    cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    num_columns = ['Age', 'Speed_of_Impact']\n",
        "    \n",
        "    # Combined feature transformer\n",
        "    features_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"numeric\", numeric_transformer, num_columns),\n",
        "            (\"categorical\", categorical_transformer, cat_columns),\n",
        "        ],\n",
        "    )\n",
        "    # Separate features and labels\n",
        "    X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "    y = df['Survived'].values\n",
        "\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    \n",
        "    # Transform train and test data\n",
        "    X_train = features_transformer.fit_transform(X_train)\n",
        "    X_test = features_transformer.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# function that trains the model\n",
        "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
        "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
        "    print(\"Training model...\")\n",
        "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "# function that evaluates the model\n",
        "def eval_model(model, X_test, y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "    mlflow.log_metric(\"Accuracy\", acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' + str(auc))\n",
        "    mlflow.log_metric(\"AUC\", auc)\n",
        "\n",
        "    # plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    # Plot the diagonal 50% line\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # Plot the FPR and TPR achieved by our model\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.savefig(\"ROC-Curve.png\")\n",
        "    mlflow.log_artifact(\"ROC-Curve.png\")    \n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
        "                        type=float, default=0.01)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/mlflow-accident-training-model-parameters.py\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Now, you can submit the script as a command job.\n",
        "\n",
        "Run the cell below to train the model. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python mlflow-accident-training-model-parameters.py --training_data accident.csv\",\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"captgt0071\",\n",
        "    display_name=\"accident-train-mlflow\",\n",
        "    experiment_name=\"accident-training\", \n",
        "    tags={\"model_type\": \"LogisticRegression\"}\n",
        "    )\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n\u001b[32mUploading src (0.02 MBs): 100%|██████████| 20498/20498 [00:00<00:00, 768153.78it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Monitor your job at https://ml.azure.com/runs/boring_date_s6d1c8rqvl?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/rg-dp100/workspaces/gabbyworkout&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1739286628711
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "In the Studio, navigate to the **accident-train-mlflow** job to explore the overview of the command job you ran:\n",
        "\n",
        "- Find the logged parameters in the **Overview** tab, under **Params**.\n",
        "- Find the logged metrics in the **Metrics** tab.\n",
        "- Find the logged artifacts in the **Images** tab (specifically for images), and in the **Outputs + logs** tab (all files)."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Autologging with MLflow\n",
        "\n",
        "Instead of using custom logging, MLflow can also automatically log any parameters, metrics, and artifacts. Autologging with MLflow requires only one line of code.\n",
        "\n",
        "Run the following cell to create the **train-model-autolog.py** script in the **src** folder. The script trains a classification model by using the **accident.csv** file in the same folder, which is passed as an argument. \n",
        "\n",
        "Review the code below to find that the script will import `mlflow` and enables autologging with the line: \n",
        "\n",
        "`mlflow.autolog()`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/mlflow-accident-training-model-autolog.py\n",
        "# import libraries\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "def main(args):\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    # train model\n",
        "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    eval_model(model, X_test, y_test)\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    print(\"Reading data...\")\n",
        "    data = pd.read_csv(path)\n",
        "    df = data.copy().dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# function that splits the data\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    # Numeric transformer pipeline\n",
        "    numeric_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"mean\"),\n",
        "        StandardScaler(),\n",
        "    )\n",
        "    \n",
        "    # Categorical transformer pipeline\n",
        "    categorical_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(drop='first')\n",
        "    )\n",
        "    \n",
        "    # Define categorical and numeric columns\n",
        "    cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    num_columns = ['Age', 'Speed_of_Impact']\n",
        "    \n",
        "    # Combined feature transformer\n",
        "    features_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"numeric\", numeric_transformer, num_columns),\n",
        "            (\"categorical\", categorical_transformer, cat_columns),\n",
        "        ],\n",
        "    )\n",
        "    # Separate features and labels\n",
        "    X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "    y = df['Survived'].values\n",
        "\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    \n",
        "    # Transform train and test data\n",
        "    X_train = features_transformer.fit_transform(X_train)\n",
        "    X_test = features_transformer.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# function that trains the model\n",
        "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
        "    print(\"Training model...\")\n",
        "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "    return model\n",
        "\n",
        "# function that evaluates the model\n",
        "def eval_model(model, X_test, y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' + str(auc))\n",
        "\n",
        "    # plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    # Plot the diagonal 50% line\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # Plot the FPR and TPR achieved by our model\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.savefig(\"ROC-Curve.png\") \n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
        "                        type=float, default=0.01)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing src/mlflow-accident-training-model-autolog.py\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Now, you can submit the script as a command job.\n",
        "\n",
        "Run the cell below to train the model. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python mlflow-accident-training-model-autolog.py --training_data accident.csv\",\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"captgt0071\",\n",
        "    display_name=\"accident-train-autolog\",\n",
        "    experiment_name=\"accident-training\"\n",
        "    )\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading src (0.02 MBs): 100%|██████████| 24452/24452 [00:00<00:00, 188724.15it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Monitor your job at https://ml.azure.com/runs/quiet_pipe_d3ykjlt677?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/rg-dp100/workspaces/gabbyworkout&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1739287307846
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "In the Studio, navigate to the **accident-train-autolog** job to explore the overview of the command job you ran:\n",
        "\n",
        "- Find the logged parameters in the **Overview** tab, under **Params**.\n",
        "- Find the logged metrics in the **Metrics** tab.\n",
        "- Find the logged artifacts in the **Images** tab (specifically for images), and in the **Outputs + logs** tab (all files, including the model files)."
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Use MLflow to view and search for experiments\n",
        "\n",
        "The Azure Machine Learning Studio is an easy-to-use UI to view and compare job runs. Alternatively, you can use MLflow to view experiment jobs. \n",
        "\n",
        "To list the jobs in the workspace, use the following command to list the experiments in the workspace:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "experiments = mlflow.search_experiments()\n",
        "for exp in experiments:\n",
        "    print(exp.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "auto-ml-class-dev\nmlflow-experiment-diabetes\nmlflow-experiment-accident\naccident-training\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1739287433711
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "To retrieve a specific experiment, you can get it by its name:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"accident-training\"\n",
        "exp = mlflow.get_experiment_by_name(experiment_name)\n",
        "print(exp)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Using an experiment name, you can retrieve all jobs of that experiment:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.search_runs(exp.experiment_id)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "To more easily compare job runs and outputs, you can configure the search to order the results. For example, the following cell orders the results by `start_time`, and only shows a maximum of `2` results: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.search_runs(exp.experiment_id, order_by=[\"start_time DESC\"], max_results=2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "You can even create a query to filter the runs. Filter query strings are written with a simplified version of the SQL `WHERE` clause. \n",
        "\n",
        "To filter, you can use two classes of comparators:\n",
        "\n",
        "- Numeric comparators (metrics): =, !=, >, >=, <, and <=.\n",
        "- String comparators (params, tags, and attributes): = and !=.\n",
        "\n",
        "Learn more about [how to track experiments with MLflow](https://learn.microsoft.com/azure/machine-learning/how-to-track-experiments-mlflow)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"metrics.AUC > 0.8 and tags.model_type = 'LogisticRegression'\"\n",
        "mlflow.search_runs(exp.experiment_id, filter_string=query)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}