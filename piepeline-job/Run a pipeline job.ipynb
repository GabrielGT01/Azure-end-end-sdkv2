{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run scripts as a pipeline job\n",
        "\n",
        "A pipeline allows you to group multiple steps into one workflow. You can build a pipeline with components. Each component reflects a Python script to run. A component is defined in a YAML file which specifies the script and how to run it. \n",
        "\n",
        "## Before you start\n",
        "\n",
        "You'll need the latest version of the  **azure-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
        "\n",
        "> **Note**:\n",
        "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-ai-ml in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.25.0)\n",
            "Requirement already satisfied: msrest>=0.6.18 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.7.1)\n",
            "Requirement already satisfied: azure-mgmt-core>=1.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.4.0)\n",
            "Requirement already satisfied: azure-storage-blob>=12.10.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (12.24.1)\n",
            "Requirement already satisfied: colorama in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.4.6)\n",
            "Requirement already satisfied: strictyaml in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.7.3)\n",
            "Requirement already satisfied: marshmallow>=3.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (3.26.1)\n",
            "Requirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.12.2)\n",
            "Requirement already satisfied: azure-common>=1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.1.28)\n",
            "Requirement already satisfied: azure-storage-file-share in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (12.20.1)\n",
            "Requirement already satisfied: pydash>=6.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (8.0.5)\n",
            "Requirement already satisfied: azure-core>=1.23.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (6.0.1)\n",
            "Requirement already satisfied: pyjwt in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (2.4.0)\n",
            "Requirement already satisfied: isodate in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.6.1)\n",
            "Requirement already satisfied: azure-storage-file-datalake>=12.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (12.18.1)\n",
            "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.66.4)\n",
            "Requirement already satisfied: azure-monitor-opentelemetry in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.6.4)\n",
            "Requirement already satisfied: jsonschema>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.23.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-ml) (1.16.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-ml) (2.32.3)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-storage-blob>=12.10.0->azure-ai-ml) (38.0.4)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.19.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.35.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (2023.12.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (24.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from marshmallow>=3.5->azure-ai-ml) (24.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest>=0.6.18->azure-ai-ml) (2024.8.30)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest>=0.6.18->azure-ai-ml) (2.0.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-urllib3~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-flask~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: azure-monitor-opentelemetry-exporter~=1.0.0b31 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.0.0b33)\n",
            "Requirement already satisfied: azure-core-tracing-opentelemetry~=1.0.0b11 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.0.0b11)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-psycopg2~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-sdk~=1.28 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-requests~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-django~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-urllib~=0.49b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-resource-detector-azure~=0.1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from strictyaml->azure-ai-ml) (2.9.0.post0)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.12.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (1.30.0)\n",
            "Requirement already satisfied: fixedint==0.1.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b31->azure-monitor-opentelemetry->azure-ai-ml) (0.1.6)\n",
            "Requirement already satisfied: psutil~=5.9 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b31->azure-monitor-opentelemetry->azure-ai-ml) (5.9.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (1.16.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-wsgi==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (1.14.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-semantic-conventions==0.51b0->opentelemetry-instrumentation-django~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (8.2.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (3.8.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-dbapi==0.51b0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation-psycopg2~=0.49b0->azure-monitor-opentelemetry->azure-ai-ml) (0.51b0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (1.26.19)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.2)\n",
            "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (2.22)\n",
            "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api<2.0.0,>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (3.19.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1739364665055
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: azure-ai-ml\n",
            "Version: 1.25.0\n",
            "Summary: Microsoft Azure Machine Learning Client Library for Python\n",
            "Home-page: https://github.com/Azure/azure-sdk-for-python\n",
            "Author: Microsoft Corporation\n",
            "Author-email: azuresdkengsysadmins@microsoft.com\n",
            "License: MIT License\n",
            "Location: /anaconda/envs/azureml_py38/lib/python3.10/site-packages\n",
            "Requires: azure-common, azure-core, azure-mgmt-core, azure-monitor-opentelemetry, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, msrest, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\n",
            "Required-by: \n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip show azure-ai-ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1739364667790
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1739364668957
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        }
      ],
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the scripts\n",
        "\n",
        "You'll build a pipeline with two steps:\n",
        "\n",
        "1. **Prepare the data**: Fix missing data and normalize the data.\n",
        "1. **Train the model**: Train a logistic regression model.\n",
        "\n",
        "Run the following cells to create the **src** folder and the two scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1739364669053
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "src folder created\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/prep-data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/prep-data.py\n",
        "# import libraries\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def main(args):\n",
        "    # read data\n",
        "    df = get_data(args.input_data)\n",
        "\n",
        "    cleaned_data = clean_data(df)\n",
        "\n",
        "    output_df = cleaned_data.to_csv((Path(args.output_data) / \"accident.csv\"), index = False)\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Count the rows and print the result\n",
        "    row_count = (len(df))\n",
        "    print('Preparing {} rows of data'.format(row_count))\n",
        "    \n",
        "    return df\n",
        "\n",
        "# function that removes missing values\n",
        "def clean_data(df):\n",
        "    df = df.dropna()\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--input_data\", dest='input_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--output_data\", dest='output_data',\n",
        "                        type=str)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/train-model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $script_folder/train-model.py\n",
        "# import libraries\n",
        "# import libraries\n",
        "import glob\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import joblib\n",
        "import mlflow\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "def main(args):\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    # train model\n",
        "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    eval_model(model, X_test, y_test)\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(data_path):\n",
        "\n",
        "    all_files = glob.glob(data_path + \"/*.csv\")\n",
        "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# function that splits the data\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "    # Numeric transformer pipeline\n",
        "    numeric_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"mean\"),\n",
        "        StandardScaler(),\n",
        "    )\n",
        "    \n",
        "    # Categorical transformer pipeline\n",
        "    categorical_transformer = make_pipeline(\n",
        "        SimpleImputer(strategy=\"most_frequent\"),\n",
        "        OneHotEncoder(drop='first')\n",
        "    )\n",
        "    \n",
        "    # Define categorical and numeric columns\n",
        "    cat_columns = ['Gender', 'Helmet_Used', 'Seatbelt_Used']\n",
        "    num_columns = ['Age', 'Speed_of_Impact']\n",
        "    \n",
        "    # Combined feature transformer\n",
        "    features_transformer = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"numeric\", numeric_transformer, num_columns),\n",
        "            (\"categorical\", categorical_transformer, cat_columns),\n",
        "        ],\n",
        "    )\n",
        "    # Separate features and labels\n",
        "    X = df[['Age', 'Gender', 'Speed_of_Impact', 'Helmet_Used', 'Seatbelt_Used']]\n",
        "    y = df['Survived'].values\n",
        "\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    \n",
        "    # Transform train and test data\n",
        "    X_train = features_transformer.fit_transform(X_train)\n",
        "    X_test = features_transformer.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# function that trains the model\n",
        "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
        "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
        "    print(\"Training model...\")\n",
        "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "    mlflow.sklearn.save_model(model, args.model_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# function that evaluates the model\n",
        "def eval_model(model, X_test, y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "    mlflow.log_metric(\"Accuracy\", acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' + str(auc))\n",
        "    mlflow.log_metric(\"area under curve\", auc)\n",
        "\n",
        "\n",
        "    # plot ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "    fig = plt.figure(figsize=(6, 4))\n",
        "    # Plot the diagonal 50% line\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # Plot the FPR and TPR achieved by our model\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.savefig(\"ROC-Curve.png\")\n",
        "    mlflow.log_artifact(\"ROC-Curve.png\")\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
        "                        type=float, default=0.01)\n",
        "    parser.add_argument(\"--model_output\", dest='model_output',\n",
        "                        type=str)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the components\n",
        "\n",
        "To define the component you need to specify:\n",
        "\n",
        "- **Metadata**: *name*, *display name*, *version*, *description*, *type* etc. The metadata helps to describe and manage the component.\n",
        "- **Interface**: *inputs* and *outputs*. For example, a model training component will take training data and the regularization rate as input, and generate a trained model file as output. \n",
        "- **Command, code & environment**: the *command*, *code* and *environment* to run the component. Command is the shell command to execute the component. Code usually refers to a source code directory. Environment could be an AzureML environment (curated or custom created), docker image or conda environment.\n",
        "\n",
        "Run the following cells to create a YAML for each component you want to run as a pipeline step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting prep-data.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile prep-data.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: prep_data\n",
        "display_name: Prepare training data\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  input_data: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  output_data:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
        "command: >-\n",
        "  python prep-data.py \n",
        "  --input_data ${{inputs.input_data}}\n",
        "  --output_data ${{outputs.output_data}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train-model.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile train-model.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_model\n",
        "display_name: Train a logistic regression model\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  training_data: \n",
        "    type: uri_folder\n",
        "  reg_rate:\n",
        "    type: number\n",
        "    default: 0.01\n",
        "outputs:\n",
        "  model_output:\n",
        "    type: mlflow_model\n",
        "code: ./src\n",
        "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
        "command: >-\n",
        "  python train-model.py \n",
        "  --training_data ${{inputs.training_data}} \n",
        "  --reg_rate ${{inputs.reg_rate}} \n",
        "  --model_output ${{outputs.model_output}} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the components\n",
        "\n",
        "Now that you have defined each component, you can load the components by referring to the YAML files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1739364669485
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "prep_data = load_component(source=parent_dir + \"./prep-data.yml\")\n",
        "train_logistic_regression = load_component(source=parent_dir + \"./train-model.yml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the pipeline\n",
        "\n",
        "After creating and loading the components, you can build the pipeline. You'll compose the two components into a pipeline. First, you'll want the `prep_data` component to run. The output of the first component should be the input of the second component `train_logistic_regression`, which will train the model.\n",
        "\n",
        "The `accident_classification` function represents the complete pipeline. The function expects one input variable: `pipeline_job_input`. A data asset was created during setup. You'll use the registered data asset as the pipeline input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1739364669638
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def accident_classification(pipeline_job_input):\n",
        "    clean_data = prep_data(input_data=pipeline_job_input)\n",
        "    train_model = train_logistic_regression(training_data=clean_data.outputs.output_data)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_transformed_data\": clean_data.outputs.output_data,\n",
        "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
        "    }\n",
        "\n",
        "pipeline_job = accident_classification(Input(type=AssetTypes.URI_FILE, path=\"azureml:accident-local:1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can retrieve the configuration of the pipeline job by printing the `pipeline_job` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1739364669816
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display_name: accident_classification\n",
            "type: pipeline\n",
            "inputs:\n",
            "  pipeline_job_input:\n",
            "    type: uri_file\n",
            "    path: azureml:accident-local:1\n",
            "outputs:\n",
            "  pipeline_job_transformed_data:\n",
            "    type: uri_folder\n",
            "  pipeline_job_trained_model:\n",
            "    type: mlflow_model\n",
            "jobs:\n",
            "  clean_data:\n",
            "    type: command\n",
            "    inputs:\n",
            "      input_data:\n",
            "        path: ${{parent.inputs.pipeline_job_input}}\n",
            "    outputs:\n",
            "      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: prep_data\n",
            "      version: '1'\n",
            "      display_name: Prepare training data\n",
            "      type: command\n",
            "      inputs:\n",
            "        input_data:\n",
            "          type: uri_file\n",
            "      outputs:\n",
            "        output_data:\n",
            "          type: uri_folder\n",
            "      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n",
            "        ${{outputs.output_data}}\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09/src\n",
            "      is_deterministic: true\n",
            "  train_model:\n",
            "    type: command\n",
            "    inputs:\n",
            "      training_data:\n",
            "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
            "    outputs:\n",
            "      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: train_model\n",
            "      version: '1'\n",
            "      display_name: Train a logistic regression model\n",
            "      type: command\n",
            "      inputs:\n",
            "        training_data:\n",
            "          type: uri_folder\n",
            "        reg_rate:\n",
            "          type: number\n",
            "          default: '0.01'\n",
            "      outputs:\n",
            "        model_output:\n",
            "          type: mlflow_model\n",
            "      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --reg_rate\n",
            "        ${{inputs.reg_rate}}  --model_output ${{outputs.model_output}} '\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09/src\n",
            "      is_deterministic: true\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pipeline_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can change any parameter of the pipeline job configuration by referring to the parameter and specifying the new value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1739364669923
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display_name: accident_classification\n",
            "type: pipeline\n",
            "inputs:\n",
            "  pipeline_job_input:\n",
            "    type: uri_file\n",
            "    path: azureml:accident-local:1\n",
            "outputs:\n",
            "  pipeline_job_transformed_data:\n",
            "    mode: upload\n",
            "    type: uri_folder\n",
            "  pipeline_job_trained_model:\n",
            "    mode: upload\n",
            "    type: mlflow_model\n",
            "jobs:\n",
            "  clean_data:\n",
            "    type: command\n",
            "    inputs:\n",
            "      input_data:\n",
            "        path: ${{parent.inputs.pipeline_job_input}}\n",
            "    outputs:\n",
            "      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: prep_data\n",
            "      version: '1'\n",
            "      display_name: Prepare training data\n",
            "      type: command\n",
            "      inputs:\n",
            "        input_data:\n",
            "          type: uri_file\n",
            "      outputs:\n",
            "        output_data:\n",
            "          type: uri_folder\n",
            "      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n",
            "        ${{outputs.output_data}}\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09/src\n",
            "      is_deterministic: true\n",
            "  train_model:\n",
            "    type: command\n",
            "    inputs:\n",
            "      training_data:\n",
            "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
            "    outputs:\n",
            "      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n",
            "    component:\n",
            "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
            "      name: train_model\n",
            "      version: '1'\n",
            "      display_name: Train a logistic regression model\n",
            "      type: command\n",
            "      inputs:\n",
            "        training_data:\n",
            "          type: uri_folder\n",
            "        reg_rate:\n",
            "          type: number\n",
            "          default: '0.01'\n",
            "      outputs:\n",
            "        model_output:\n",
            "          type: mlflow_model\n",
            "      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --reg_rate\n",
            "        ${{inputs.reg_rate}}  --model_output ${{outputs.model_output}} '\n",
            "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
            "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09/src\n",
            "      is_deterministic: true\n",
            "settings:\n",
            "  default_datastore: azureml:workspaceblobstore\n",
            "  default_compute: azureml:captgt0071\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# change the output mode\n",
        "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\n",
        "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\"\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute = \"captgt0071\"\n",
        "# set pipeline level datastore\n",
        "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
        "\n",
        "# print the pipeline job again to review the changes\n",
        "print(pipeline_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submit the pipeline job\n",
        "\n",
        "Finally, when you've built the pipeline and configured the pipeline job to run as required, you can submit the pipeline job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1739364678414
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading src (0.01 MBs): 100%|██████████| 10005/10005 [00:00<00:00, 439312.53it/s]\n",
            "\u001b[39m\n",
            "\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.MLFlowModelJobOutput'> and will be ignored\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_diabetes</td><td>sharp_raisin_k4001tk5gt</td><td>pipeline</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/sharp_raisin_k4001tk5gt?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/gabby&amp;tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
            ],
            "text/plain": [
              "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f718449d5a0>}, 'outputs': {'pipeline_job_transformed_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f718449d720>, 'pipeline_job_trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f718449d570>}, 'jobs': {}, 'component': PipelineComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f71845e1690>, 'version': '1', 'schema': None, 'type': 'pipeline', 'display_name': 'accident_classification', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_transformed_data': {}, 'pipeline_job_trained_model': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'clean_data': Command({'parameters': {}, 'init': False, 'name': 'clean_data', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f7184355810>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_data': '${{parent.outputs.pipeline_job_transformed_data}}'}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f7184357670>}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f7184354b80>}, 'component': 'azureml_anonymous:91762ccb-d14e-423d-9dd9-94d957310d84', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '6bcbabf0-43d2-4520-9efd-7959d870b75f', 'source': 'YAML.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'train_model': Command({'parameters': {}, 'init': False, 'name': 'train_model', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f71845e1c30>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'training_data': '${{parent.jobs.clean_data.outputs.output_data}}'}, 'job_outputs': {'model_output': '${{parent.outputs.pipeline_job_trained_model}}'}, 'inputs': {'training_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f71845e1fc0>}, 'outputs': {'model_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f7184355660>}, 'component': 'azureml_anonymous:3fd1c267-ffda-4f57-9314-afbaa53ec031', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'd5ad8166-6258-4f9f-b924-b101f326e90d', 'source': 'YAML.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'YAML.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'NotStarted', 'log_files': None, 'name': 'sharp_raisin_k4001tk5gt', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/mlw-dp100/providers/Microsoft.MachineLearningServices/workspaces/gabby/jobs/sharp_raisin_k4001tk5gt', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/captgt0071/code/Users/captgt007/azure-ml-labs/Labs/09', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f718449dff0>, 'serialize': <msrest.serialization.Serializer object at 0x7f718449f220>, 'display_name': 'accident_classification', 'experiment_name': 'pipeline_diabetes', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourceGroups/mlw-dp100/providers/Microsoft.MachineLearningServices/workspaces/gabby?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/sharp_raisin_k4001tk5gt?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/mlw-dp100/workspaces/gabby&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"pipeline_accident\"\n",
        ")\n",
        "pipeline_job"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f2b2cd046deda8eabef1e765a11d0ec9aa9bd1d31d56ce79c815a38c323e14ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
